<!--template source: https://github.com/daviddao/daviddao.github.io-->
<!DOCTYPE html>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Christoph Feichtenhofer</title>
  <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
  <link href="css/style.css" rel="stylesheet">
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300,500' rel='stylesheet' type='text/css'>

  <style type="text/css"></style>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-58616069-1', 'auto');
  ga('send', 'pageview');
  
</script>

</head>

<body>


<div id="header" class="bg2"> </div>

  <div id="headerblob">
    <img src="./img/chris.jpg" class="img-circle imgme"  width="130px">
    <div id="headertext">
      <div id="htname" style="color:white">Christoph Feichtenhofer</div>
      <div id="htdesc" style="color:white">PhD Student, Graz University of Technology</div>
      <div id="htem" style="color:white">feichtenhofer _at_ tugraz.at</div>
      <div id="icons">
        <div class="svgico">
          <a href="https://plus.google.com/117756895023424530482/posts"><img src="./img/80-google-plus.svg"></a>
        </div>
        <div class="svgico">
          <a href="https://github.com/feichtenhofer"><img src="./img/octocat.svg" width="56px"></a>
		</div>
        <div class="svgico">
          <a href="http://scholar.google.com/citations?user=UxuqG1EAAAAJ"><img src="./img/gscholar.svg" width="50px"></a>		  
        </div>
      </div>
    </div>
  </div>


<br><br>

<div class="container quote">
  <h3>Research Statement</h3>

  <blockquote>
<p class="text-left">My research interests are in the fields of computer vision and machine learning, with a focus on learning effective video representations for dynamic scene understanding. In particular, I plan to explore computational theories that represent spatiotemporal visual information, within a confluence of machine vision and learning. I aim to find efficient solutions for problems that are grounded in applications such as video search and retrieval. </p>

  </blockquote>
</div>
<div class="container">
  <h3>Education</h3>
  <div id="timeline">
    <div class="timelineitem">
      <div class="tdate">03/2015 - 06/2015</div>
      <div class="ttitle">University of Oxford, Oxford, UK: Visiting Researcher  </div>
	  <div class="tdesc"> Worked with Prof. Andrew Zisserman in the field of Human Action Recognition </div>
    </div>
	<div class="timelineitem">
      <div class="tdate">06/2014</div>
      <div class="ttitle">York University, Toronto, Canada: Visiting Researcher  </div>
	  <div class="tdesc"> Worked with Prof. Richard P. Wildes in the field of Dynamic Scene Recognition </div>
    </div>
    <div class="timelineitem">
      <div class="tdate">Since 2014</div>
      <div class="ttitle">Graz University of Technology: PhD Student</div>
      <div class="tdesc">Specialization on <span class="thigh">Learning Effective Spatiotemporal Representations for Recognition in Computer Vision </span></div>
    </div>
	<div class="timelineitem">
      <div class="tdate">2012 - 2013</div>
      <div class="ttitle">Graz University of Technology: Master's Degree </div>
      <div class="tdesc">Thesis: <span class="thigh">Dynamic Scene Recognition with Oriented Spacetime Energies</span></div>
    </div>
    <div class="timelineitem">
      <div class="tdate">03/2013 - 06/2013</div>
      <div class="ttitle">York University, Toronto, Canada: Visiting Researcher  </div>
	  <div class="tdesc"> Worked with Prof. Richard P. Wildes in the field of Dynamic Scene Recognition </div>
    </div>
    <div class="timelineitem">
      <div class="tdate">2008 - 2011
      </div>
      <div class="ttitle">Graz University of Technology: Bachelor's Degree </div>
      <div class="tdesc">Thesis: <span class="thigh">No-Reference Sharpness Metric based on Local Gradient Analysis</span></div>
    </div>
  </div>
</div>



<hr class="soft">

<div class="container">
  <h2>Publications</h2>
  <div id="pubs">

   	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="pubs/cvpr16_teaser.jpg"><br><br>
			<img src="pubs/cvpr16_architecture.jpg">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Convolutional Two-Stream Network Fusion for Video Action Recognition</div>
            <div class="pubd">Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters; (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.
			</div>
            <div class="puba">Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman</div>
            <div class="pubv">in Proc. CVPR 2016 </div>
            <div class="publ">
              <ul>
                <li><a href="pubs/Feichtenhofer_Two_Stream_Fusion_2016_CVPR.pdf">paper</a></li>			
              </ul>
			  
            </div>
          </div>
        </div>
      </div>
	</div>   
	
 	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="pubs/DPCF_intro_square.jpg">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Dynamic Scene Recognition with Complementary Spatiotemporal Features</div>
            <div class="pubd">This paper presents Dynamically Pooled Complementary Features, a unified approach to dynamic scene recognition that analyzes a short video clip in terms of its spatial, temporal and color properties.  The complementarity of these properties is preserved through all main steps of processing, including primitive feature extraction, coding and pooling. In the feature extraction step, spatial orientations capture static appearance, spatiotemporal oriented energies capture image dynamics and color statistics capture chromatic information. Subsequently, primitive features are encoded into a mid-level representation that has been learned for the task of dynamic scene recognition. Finally, a novel dynamic spacetime pyramid is introduced. This  dynamic pooling approach can handle both global as well as local motion by adapting to the temporal structure, as guided by pooling energies. The resulting system provides online recognition of dynamic scenes that is thoroughly evaluated on the two current benchmark datasets and yields best results to date on both datasets. In-depth analysis reveals  the benefits of explicitly modeling feature complementarity in combination with the dynamic spacetime pyramid, indicating that this unified approach should be well-suited to many areas of video analysis.
			</div>
            <div class="puba">Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes</div>
            <div class="pubv">in PAMI 2016 </div>
            <div class="publ">
              <ul>
                <li><a href="pubs/FeichtenhoferPinzWildesPAMI2016.pdf">paper</a></li>			
              </ul>
			  
            </div>
          </div>
        </div>
      </div>
	</div>   
   
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
		  <table width="100%">
			  <tr>
				<td align="center" valign="bottom">	<video autoplay loop> <source src="pubs/Davis_Love_III__Beautiful_swing!_golf_f_cm_np1_ri_med_0.mp4" type="video/mp4"> </video></td>
			  </tr>
			  <tr>
				<td align="center" valign="bottom">   <video autoplay loop> <source src="pubs/Davis_Love_III__Beautiful_swing!_golf_f_cm_np1_ri_med_sal.mp4" type="video/mp4"> </video></td>
			  </tr>
		  </table>
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Dynamically Encoded Actions based on Spacetime Saliency</div>
            <div class="pubd">Human actions typically occur over a well localized extent in both space and time. Similarly, as typically captured in video, human actions have small spatiotemporal support in image space. This paper capitalizes on these observations by weighting feature pooling for action recognition over those areas within a video where actions are most likely to occur. To enable this operation, we define a novel measure of spacetime saliency. The measure relies on two observations regarding foreground motion of human actors: They typically exhibit motion that contrasts with that of their surrounding region and they are spatially compact. By using the resulting definition of saliency during feature pooling we show that action recognition performance achieves state-of-the-art levels on three widely considered action recognition datasets. Our saliency weighted pooling can be applied to essentially any locally defined features and encodings thereof. Additionally, we demonstrate that inclusion of locally aggregated spatiotemporal energy features, which efficiently result as a by-product of the saliency computation, further boosts performance over reliance on standard action recognition features alone.</div>
            <div class="puba">Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes</div>
            <div class="pubv">in Proc. CVPR 2015 </div>
            <div class="publ">
              <ul>
                <li><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Feichtenhofer_Dynamically_Encoded_Actions_2015_CVPR_paper.pdf">paper</a></li>			
              </ul>
			  
            </div>
          </div>
        </div>
      </div>
    </div>
	
  
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
		  <video autoplay loop>
			  <source src="pubs/windmill_energies.mp4" type="video/mp4">
				<img src="pubs/windmill_energies.jpg" title="Sorry, your browser doesn't support embedded videos." />
			</video>
			
			
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Bags of Spacetime Energies for Dynamic Scene Recognition</div>
            <div class="pubd">This paper presents a unified bag of visual word (BoW) framework for dynamic scene recognition. The approach builds on primitive features that uniformly capture spatial and temporal orientation structure of the imagery (e.g., video), as extracted via application of a bank of spatiotemporally oriented filters. Various feature encoding techniques are investigated to abstract the primitives to an intermediate representation that is best suited to dynamic scene representation. Further, a novel approach to adaptive pooling of the encoded features is presented that captures spatial layout of the scene even while being robust to situations where camera motion and scene dynamics are confounded. The resulting overall approach has been evaluated on two standard, publically available dynamic scene datasets. The results show that in comparison to a representative set of alternatives, the proposed approach outperforms the previous state-of-the-art in classification accuracy by 10%.</div>
            <div class="puba">Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes</div>
            <div class="pubv">in Proc. CVPR 2014 </div>
            <div class="publ">
              <ul>
                <li><a href="pubs/Feichtenhofer_Bags_of_Spacetime_2014_CVPR_paper.pdf">paper</a></li>
			<li><a href="#" onClick="BoSESpotlight=window.open('pubs/BoSE_spotlight.mp4','BoSESpotlight','toolbar=no, location=no, directories=no,status=no, menubar=no,scrollbars=no, resizable=true'); return false;">spotlight video</a></li>
			
              </ul>
			  
            </div>
          </div>
        </div>
      </div>
    </div>
	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="pubs/strf.PNG">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Spacetime Forests with Complementary
									Features for Dynamic Scene Recognition</div>
            <div class="pubd">This paper presents spacetime forests defined over complementary spatial and temporal
								features for recognition of naturally occurring dynamic scenes. The approach
								improves on the previous state-of-the-art in both classification and execution rates. A
								particular improvement is with increased robustness to camera motion, where previous
								approaches have experienced difficulty. There are three key novelties in the approach.
								First, a novel spacetime descriptor is employed that exploits the complementary nature
								of spatial and temporal information, as inspired by previous research on the role of orientation
								features in scene classification. Second, a forest-based classifier is used to learn
								a multi-class representation of the feature distributions. Third, the video is processed in
								temporal slices with scale matched preferentially to scene dynamics over camera motion.
								Slicing allows for temporal alignment to be handled as latent information in the classifier
								and for efficient, incremental processing. The integrated approach is evaluated empirically
								on two publically available datasets to document its outstanding performance.
			</div>
            <div class="puba">Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes</div>
            <div class="pubv">in Proc. BMVC 2014 </div>
            <div class="publ">
              <ul>
                <li><a href="pubs/spacetime_forests_BMVC14.pdf">paper</a></li>			
              </ul>
			  
            </div>
          </div>
        </div>
      </div>
	</div>  
	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="pubs/rfid_vision_block_diagram.svg"><br><br>
			<img src="pubs/rfid_SP-ST.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Fusing RFID and Computer Vision for Probabilistic Tag Localization</div>
            <div class="pubd">The combination of RFID and computer vision
						systems is an effective approach to mitigate the limited tag
						localization capabilities of current RFID deployments. In this
						paper, we present a hybrid RFID and computer vision system
						for localization and tracking of RFID tags. The proposed system
						combines the information from the two complementary sensor
						modalities in a probabilistic manner and provides a high degree
						of ﬂexibility. In addition, we introduce a robust data association
						method which is crucial for the application in practical scenarios.
						To demonstrate the performance of the proposed system, we
						conduct a series of experiments in an article surveillance setup.
						This is a frequent application for RFID systems in retail where
						previous approaches solely based on RFID localization have
						difﬁculties due to false alarms triggered by stationary tags. Our
						evaluation shows that the fusion of RFID and computer vision
						provides robustness to false positive observations and allows for
						a reliable system operation.</div>
            <div class="puba">Michael Goller, Christoph Feichtenhofer, Axel Pinz</div>
            <div class="pubv">in Proc. IEEE RFID 2014 </div>
            <div class="publ">
              <ul>
                <li><a href="pubs/Goller_fusing_RFID_with_Vision_2014.pdf">paper</a></li>			
              </ul>
			  
            </div>
          </div>
        </div>
      </div>
    </div>
	  <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="pubs/stm_hom.jpg">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Spatio-Temporal Good Features to Track</div>
            <div class="pubd">This paper presents two fundamental contributions that
								can be very useful for any autonomous system that requires
								point correspondences for visual odometry. First,
								the Spatio-Temporal Monitor (STM) is an efficient method
								to identify good features to track by monitoring their spatiotemporal
								(x-y-t) appearance without any assumptions about
								motion or geometry. The STM may be used with any spatial
								(x-y) descriptor, but it performs best when combined with
								our second contribution, the Histogram of Oriented Magnitudes
								(HOM) descriptor, which is based on spatially oriented
								multiscale filter magnitudes. To fulfil the real-time requirements
								of autonomous applications, the same descriptor
								can be used for both, track generation and monitoring,
								to identify low-quality feature tracks at virtually no additional
								computational cost. Our extensive experimental validation
								on a challenging public dataset demonstrates the
								excellent performance of STM and HOM, where we significantly
								outperform the well known “Good Features to
								Track” method and show that our proposed feature quality
								measure highly correlates with the accuracy in structure
								and motion estimation.
			</div>
            <div class="puba">Christoph Feichtenhofer, Axel Pinz</div>
            <div class="pubv">in Proc. CVAD, ICCV 2013 </div>
            <div class="publ">
              <ul>
                <li><a href="pubs/Feichtenhofer_Bags_of_Spacetime_2014_CVPR_paper.pdf">paper</a></li>			
              </ul>
			  
            </div>
          </div>
        </div>
      </div>
	</div>  
	
	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
			<img src="pubs/sharpness/monarchblocks10pct.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">A Perceptual Image Sharpness Metric Based on
								Local Edge Gradient Analysis</div>
            <div class="pubd">In this letter, a no-reference perceptual sharpness
						metric based on a statistical analysis of local edge gradients is
						presented. The method takes properties of the human visual
						system into account. Based on perceptual properties, a relationship
						between the extracted statistical features and the metric
						score is established to form a Perceptual Sharpness Index (PSI). A
						comparison with state-of-the-art metrics shows that the proposed
						method correlates highly with human perception and exhibits low
						computational complexity. In contrast to existing metrics, the PSI
						performs well for a wide range of blurriness and shows a high
						degree of invariance for different image contents.</div>
            <div class="puba">Christoph Feichtenhofer, Hannes Fassold, Peter Schallauer</div>
            <div class="pubv">in IEEE Signal Processing Letters 2013 </div>
            <div class="publ">
              <ul>
                <li><a href="pubs/Feichtenhofer_Image_Sharpness_Metric_IEEE_SPL_2013.pdf">paper</a></li>			
                <li><a href="https://github.com/feichtenhofer/PSI">code</a></li>			
              </ul>
			  
            </div>
          </div>
        </div>
      </div>
	
	
    </div>
  </div>
</div>



<hr class="soft">

<div class="container">
  <h2>Teaching</h2>
  <div class="ctr">
    <div class="pubt">Image and Video Understanding (winter 2014-2015, together with Prof. Axel Pinz)</div>
      <img src="./pubs/teaching/dynamic_scene_understanding.png" >
    </a>
  </div>
  <h3>My Lectures cover</h3>
  <div class="ctr">
    <div class="hht"><a href="pubs/teaching/IVU_Convolutional_Filtering_and_Thinking_in_Frequency.pdf">Convolutional Filtering and Thinking in Frequency </a> </div>
  </div>
  <div class="ctr">
    <div class="hht"><a href="pubs/teaching/IVU_Convolutional_Networks_and_Video_Representations.pdf">Convolutional Networks and Video Representations</a> </div>
  </div>
</div>




</body></html>